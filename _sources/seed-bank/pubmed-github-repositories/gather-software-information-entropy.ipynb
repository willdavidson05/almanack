{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d35a184-789b-4f59-872d-b0fe94f7934e",
   "metadata": {},
   "source": [
    "# PubMed article GitHub repository software information entropy\n",
    "\n",
    "This notebook focuses on gathering PubMed article repository software information entropy data.\n",
    "\n",
    "PubMed article GitHub repositories are extracted using the PubMed API to query for GitHub links within article abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c6931",
   "metadata": {},
   "source": [
    "## PubMed GitHub Repositories Analysis\n",
    "\n",
    "The following code is used to analyze PubMed article GitHub repository software information entropy using existing methods from the Software Gardening Almanack package.\n",
    "Each repository is processed in parallel through batches written to Parquet files which in aggregate compose a full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e6e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from almanack import process_repo_for_analysis\n",
    "\n",
    "\n",
    "def repository_analysis():\n",
    "    \"\"\"\n",
    "    Analyzes PudMed GitHub repositories to compute normalized information entropy\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the results for the last batch of repositories processed.\n",
    "                      Each row includes:\n",
    "                      - 'Repository URL': The URL of the repository.\n",
    "                      - 'Normalized Total Entropy': The normalized entropy value for the repository.\n",
    "                      - 'Date of First Commit': The date of the first commit in the repository.\n",
    "                      - 'Date of Last Commit': The date of the most recent commit in the repository.\n",
    "                      - 'Time of Existence (days)': The number of days between the first and last commit.\n",
    "\n",
    "    Notes:\n",
    "        - Reads repository URLs from a Parquet file located at \"../../../tests/data/examples/pubmed/pubmed_github_links.parquet\".\n",
    "        - Processes repositories in batches of 500 to manage the large datasets efficiently.\n",
    "        - Results are saved in Parquet files named `repository_analysis_results_batch_X.parquet`, where X is the batch number.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(\"gather-pubmed-repos/pubmed_github_links.parquet\")\n",
    "\n",
    "    repo_urls = df[\"github_link\"].tolist()\n",
    "\n",
    "    batch_size = 500\n",
    "    total_repos = len(repo_urls)\n",
    "    repo_count = 0\n",
    "    batch_number = 1\n",
    "\n",
    "    # Process repositories in batches\n",
    "    for start in range(0, total_repos, batch_size):\n",
    "        # Determine the end index for the current batch\n",
    "        end = min(start + batch_size, total_repos)\n",
    "        # Extract the URLs for the current batch\n",
    "        batch_urls = repo_urls[start:end]\n",
    "\n",
    "        # Create a ProcessPoolExecutor to process repositories in parallel\n",
    "        with ProcessPoolExecutor(max_workers=16) as executor:\n",
    "            # Submit tasks to process each repository URL and store future results\n",
    "            futures = {\n",
    "                executor.submit(process_repo_for_analysis, repo_url): repo_url\n",
    "                for repo_url in batch_urls\n",
    "            }\n",
    "            batch_results = []\n",
    "            # Iterate over completed futures as they finish\n",
    "            for future in as_completed(futures):\n",
    "                repo_count += 1\n",
    "                # Get the repository URL corresponding to the completed future\n",
    "                repo_url = futures[future]\n",
    "                try:\n",
    "                    (\n",
    "                        normalized_total_entropy,\n",
    "                        first_commit_date,\n",
    "                        most_recent_commit_date,\n",
    "                        time_of_existence,\n",
    "                    ) = future.result()\n",
    "                    print(\n",
    "                        f\"Repository {repo_count}: {repo_url}, {normalized_total_entropy}\"\n",
    "                    )\n",
    "                    batch_results.append(\n",
    "                        [\n",
    "                            repo_url,\n",
    "                            normalized_total_entropy,\n",
    "                            first_commit_date,\n",
    "                            most_recent_commit_date,\n",
    "                            time_of_existence,\n",
    "                        ]\n",
    "                    )\n",
    "                except Exception:\n",
    "                    # Handle any exceptions by appending a row with None values\n",
    "                    batch_results.append([repo_url, None, None, None, None])\n",
    "\n",
    "        # Create DataFrame for the current batch\n",
    "        df_results = pd.DataFrame(\n",
    "            batch_results,\n",
    "            columns=[\n",
    "                \"Repository URL\",\n",
    "                \"Normalized Total Entropy\",\n",
    "                \"Date of First Commit\",\n",
    "                \"Date of Last Commit\",\n",
    "                \"Time of Existence (days)\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Save batch results to Parquet file\n",
    "        batch_filename = f\"repository_analysis_results_batch_{batch_number}.parquet\"\n",
    "        df_results.to_parquet(batch_filename)\n",
    "        print(f\"Batch {batch_number} results saved to {batch_filename}\")\n",
    "\n",
    "        # Increment batch number\n",
    "        batch_number += 1\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "df_results = repository_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
